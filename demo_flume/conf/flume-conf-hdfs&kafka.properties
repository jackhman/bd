# 组装agent
a1.sources = src_1
a1.channels = ch_1 ch_2
a1.sinks = sink_1 sink_2

# 配置 source：从目录中读取文件，不能读取正在写入的文件
a1.sources.src_1.type = spooldir
a1.sources.src_1.channels = ch_1 ch_2
a1.sources.src_1.spoolDir = /data/flume/log/
a1.sources.src_1.includePattern=^.*$
a1.sources.src_1.ignorePattern=^.*log$
a1.sources.src_1.deletePolicy= never
a1.sources.src_1.fileHeader = true
## 增加时间header
a1.sources.src_1.interceptors = i1
a1.sources.src_1.interceptors.i1.type = timestamp

# 配置 channel：缓存到文件中
a1.channels.ch_1.type = memory
a1.channels.ch_1.capacity = 10000
a1.channels.ch_1.transactionCapacity = 10000


# 配置 sink：保存到hdfs中
a1.sinks.sink_1.channel = ch_1
a1.sinks.sink_1.type = hdfs
a1.sinks.sink_1.hdfs.path = hdfs://gp-bd-master01:8020/user/george/flume/accesslog/%Y-%m-%d
a1.sinks.sink_1.hdfs.filePrefix = logs
a1.sinks.sink_1.hdfs.rollInterval = 10
a1.sinks.sink_1.hdfs.rollSize = 0
a1.sinks.sink_1.hdfs.rollCount = 0
a1.sinks.sink_1.hdfs.batchSize = 100
a1.sinks.sink_1.hdfs.writeFormat = Text
a1.sinks.sink_1.hdfs.minBlockReplicas = 1

a1.channels.ch_2.type = memory
a1.channels.ch_2.capacity = 10000
a1.channels.ch_2.transactionCapacity = 10000

a1.sinks.sink_2.channel = ch_2
a1.sinks.sink_2.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.sink_2.kafka.topic = mytopic
a1.sinks.sink_2.kafka.bootstrap.servers = gp-bd-master01:9092
a1.sinks.sink_2.kafka.flumeBatchSize = 20
a1.sinks.sink_2.kafka.producer.acks = 1
a1.sinks.sink_2.kafka.producer.linger.ms = 1
a1.sinks.sink_2.kafka.producer.compression.type = snappy
